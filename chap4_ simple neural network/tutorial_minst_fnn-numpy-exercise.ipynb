{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo numpy based auto differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Matmul:\n",
    "    def __init__(self):\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x, W):\n",
    "        h = np.matmul(x, W)\n",
    "        self.mem={'x': x, 'W':W}\n",
    "        return h\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        x: shape(N, d)\n",
    "        w: shape(d, d')\n",
    "        grad_y: shape(N, d')\n",
    "        '''\n",
    "        x = self.mem['x']\n",
    "        W = self.mem['W']\n",
    "        \n",
    "        ####################\n",
    "        '''计算矩阵乘法的对应的梯度'''\n",
    "        grad_x = np.matmul(grad_y, W.T)\n",
    "        grad_W = np.matmul(x.T, grad_y)\n",
    "\n",
    "        ####################\n",
    "        return grad_x, grad_W\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mem['x']=x\n",
    "        return np.where(x > 0, x, np.zeros_like(x))\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        grad_y: same shape as x\n",
    "        '''\n",
    "        ####################\n",
    "        '''计算relu 激活函数对应的梯度'''\n",
    "        grad_x = np.where(self.mem['x'] > 0, grad_y, np.zeros_like(grad_y))\n",
    "        ####################\n",
    "        return grad_x\n",
    "    \n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    '''\n",
    "    softmax over last dimention\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-12\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: shape(N, c)\n",
    "        '''\n",
    "        x_exp = np.exp(x)\n",
    "        partition = np.sum(x_exp, axis=1, keepdims=True)\n",
    "        out = x_exp/(partition+self.epsilon)\n",
    "        \n",
    "        self.mem['out'] = out\n",
    "        self.mem['x_exp'] = x_exp\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        grad_y: same shape as x\n",
    "        '''\n",
    "        s = self.mem['out']\n",
    "        sisj = np.matmul(np.expand_dims(s,axis=2), np.expand_dims(s, axis=1)) # (N, c, c)\n",
    "        g_y_exp = np.expand_dims(grad_y, axis=1)\n",
    "        tmp = np.matmul(g_y_exp, sisj) #(N, 1, c)\n",
    "        tmp = np.squeeze(tmp, axis=1)\n",
    "        tmp = -tmp+grad_y*s \n",
    "        return tmp\n",
    "    \n",
    "class Log:\n",
    "    '''\n",
    "    softmax over last dimention\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-12\n",
    "        self.mem = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: shape(N, c)\n",
    "        '''\n",
    "        out = np.log(x+self.epsilon)\n",
    "        \n",
    "        self.mem['x'] = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_y):\n",
    "        '''\n",
    "        grad_y: same shape as x\n",
    "        '''\n",
    "        x = self.mem['x']\n",
    "        \n",
    "        return 1./(x+1e-12) * grad_y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 2.35008944,  1.11852029,  2.41787226, -1.38881918, -0.66852192,\n",
      "        -1.05765814],\n",
      "       [ 2.35008944,  1.11852029,  2.41787226, -1.38881918, -0.66852192,\n",
      "        -1.05765814],\n",
      "       [ 2.35008944,  1.11852029,  2.41787226, -1.38881918, -0.66852192,\n",
      "        -1.05765814],\n",
      "       [ 2.35008944,  1.11852029,  2.41787226, -1.38881918, -0.66852192,\n",
      "        -1.05765814],\n",
      "       [ 2.35008944,  1.11852029,  2.41787226, -1.38881918, -0.66852192,\n",
      "        -1.05765814]]), array([[-0.40082939, -0.40082939, -0.40082939, -0.40082939],\n",
      "       [ 0.96438947,  0.96438947,  0.96438947,  0.96438947],\n",
      "       [ 1.51725789,  1.51725789,  1.51725789,  1.51725789],\n",
      "       [ 0.48249143,  0.48249143,  0.48249143,  0.48249143],\n",
      "       [ 1.24160563,  1.24160563,  1.24160563,  1.24160563],\n",
      "       [ 0.54556466,  0.54556466,  0.54556466,  0.54556466]]))\n",
      "tf.Tensor(\n",
      "[[ 2.35008944  1.11852029  2.41787226 -1.38881918 -0.66852192 -1.05765814]\n",
      " [ 2.35008944  1.11852029  2.41787226 -1.38881918 -0.66852192 -1.05765814]\n",
      " [ 2.35008944  1.11852029  2.41787226 -1.38881918 -0.66852192 -1.05765814]\n",
      " [ 2.35008944  1.11852029  2.41787226 -1.38881918 -0.66852192 -1.05765814]\n",
      " [ 2.35008944  1.11852029  2.41787226 -1.38881918 -0.66852192 -1.05765814]], shape=(5, 6), dtype=float64)\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "tf.Tensor(\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]], shape=(5, 6), dtype=float64)\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "[[-2.33061443e-03  2.80920952e-03 -7.91782744e-07 -2.08263475e-08\n",
      "  -5.81104525e-05 -4.19672037e-04]\n",
      " [ 1.62008860e-06  2.26831912e-03 -8.32390956e-10 -1.11476227e-04\n",
      "  -2.95241692e-08 -2.15843263e-03]\n",
      " [-3.77886673e-05 -4.18863521e-07 -1.26623653e-08  2.56566784e-04\n",
      "  -2.16000510e-04 -2.34608105e-06]\n",
      " [-8.81833128e-05 -1.79270868e-03 -1.27971359e-03 -1.36983463e-03\n",
      "  -1.90944814e-03  6.43988836e-03]\n",
      " [ 1.42964796e-01 -1.37650447e-01 -9.15957248e-04 -6.71098596e-04\n",
      "  -3.45241289e-03 -2.74880464e-04]]\n",
      "tf.Tensor(\n",
      "[[-2.33061443e-03  2.80920952e-03 -7.91782744e-07 -2.08263475e-08\n",
      "  -5.81104525e-05 -4.19672037e-04]\n",
      " [ 1.62008860e-06  2.26831912e-03 -8.32390956e-10 -1.11476227e-04\n",
      "  -2.95241692e-08 -2.15843263e-03]\n",
      " [-3.77886673e-05 -4.18863521e-07 -1.26623653e-08  2.56566784e-04\n",
      "  -2.16000510e-04 -2.34608105e-06]\n",
      " [-8.81833128e-05 -1.79270868e-03 -1.27971359e-03 -1.36983463e-03\n",
      "  -1.90944814e-03  6.43988836e-03]\n",
      " [ 1.42964796e-01 -1.37650447e-01 -9.15957248e-04 -6.71098596e-04\n",
      "  -3.45241289e-03 -2.74880464e-04]], shape=(5, 6), dtype=float64)\n",
      "[[ 0.         -0.96463328 -0.         -0.          0.          0.        ]\n",
      " [-0.31191407 -2.81153135  0.          0.          0.         -0.        ]\n",
      " [ 0.          0.         -0.         -1.19371957  0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.         -1.27500921]\n",
      " [-8.99315335  0.         -0.         -0.          0.         -0.        ]]\n",
      "tf.Tensor(\n",
      "[[ 0.         -0.96463328 -0.         -0.          0.          0.        ]\n",
      " [-0.31191407 -2.81153135  0.          0.          0.         -0.        ]\n",
      " [ 0.          0.         -0.         -1.19371957  0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.         -1.27500921]\n",
      " [-8.99315335  0.         -0.         -0.          0.         -0.        ]], shape=(5, 6), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31573\\AppData\\Local\\Temp\\ipykernel_24760\\1531340013.py:94: RuntimeWarning: invalid value encountered in log\n",
      "  out = np.log(x+self.epsilon)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = np.random.normal(size=[5, 6])\n",
    "W = np.random.normal(size=[6, 4])\n",
    "aa = Matmul()\n",
    "out = aa.forward(x, W) # shape(5, 4)\n",
    "grad = aa.backward(np.ones_like(out))\n",
    "print (grad)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x, W = tf.constant(x), tf.constant(W)\n",
    "    tape.watch(x)\n",
    "    y = tf.matmul(x, W)\n",
    "    loss = tf.reduce_sum(y)\n",
    "    grads = tape.gradient(loss, x)\n",
    "    print (grads)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x = np.random.normal(size=[5, 6])\n",
    "aa = Relu()\n",
    "out = aa.forward(x) # shape(5, 4)\n",
    "grad = aa.backward(np.ones_like(out))\n",
    "print (grad)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x= tf.constant(x)\n",
    "    tape.watch(x)\n",
    "    y = tf.nn.relu(x)\n",
    "    loss = tf.reduce_sum(y)\n",
    "    grads = tape.gradient(loss, x)\n",
    "    print (grads)\n",
    "\n",
    "import tensorflow as tf\n",
    "x = np.random.normal(size=[5, 6], scale=5.0, loc=1)\n",
    "label = np.zeros_like(x)\n",
    "label[0, 1]=1.\n",
    "label[1, 0]=1\n",
    "label[1, 1]=1\n",
    "label[2, 3]=1\n",
    "label[3, 5]=1\n",
    "label[4, 0]=1\n",
    "print(label)\n",
    "aa = Softmax()\n",
    "out = aa.forward(x) # shape(5, 6)\n",
    "grad = aa.backward(label)\n",
    "print (grad)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x= tf.constant(x)\n",
    "    tape.watch(x)\n",
    "    y = tf.nn.softmax(x)\n",
    "    loss = tf.reduce_sum(y*label)\n",
    "    grads = tape.gradient(loss, x)\n",
    "    print (grads)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x = np.random.normal(size=[5, 6])\n",
    "aa = Log()\n",
    "out = aa.forward(x) # shape(5, 4)\n",
    "grad = aa.backward(label)\n",
    "print (grad)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x= tf.constant(x)\n",
    "    tape.watch(x)\n",
    "    y = tf.math.log(x)\n",
    "    loss = tf.reduce_sum(y*label)\n",
    "    grads = tape.gradient(loss, x)\n",
    "    print (grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.          64.58090621   0.           0.           0.\n",
      "    0.        ]\n",
      " [  4.56789213   0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.           4.08261448   0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "  172.72369723]\n",
      " [  6.22913429   0.           0.           0.           0.\n",
      "    0.        ]]\n",
      "----------------------------------------\n",
      "[[  0.          64.58090622   0.           0.           0.\n",
      "    0.        ]\n",
      " [  4.56789213   0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.           4.08261448   0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "  172.72369726]\n",
      " [  6.22913429   0.           0.           0.           0.\n",
      "    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "label = np.zeros_like(x)\n",
    "label[0, 1]=1.\n",
    "label[1, 0]=1\n",
    "label[2, 3]=1\n",
    "label[3, 5]=1\n",
    "label[4, 0]=1\n",
    "\n",
    "x = np.random.normal(size=[5, 6])\n",
    "W1 = np.random.normal(size=[6, 5])\n",
    "W2 = np.random.normal(size=[5, 6])\n",
    "\n",
    "mul_h1 = Matmul()\n",
    "mul_h2 = Matmul()\n",
    "relu = Relu()\n",
    "softmax = Softmax()\n",
    "log = Log()\n",
    "\n",
    "h1 = mul_h1.forward(x, W1) # shape(5, 4)\n",
    "h1_relu = relu.forward(h1)\n",
    "h2 = mul_h2.forward(h1_relu, W2)\n",
    "h2_soft = softmax.forward(h2)\n",
    "h2_log = log.forward(h2_soft)\n",
    "\n",
    "\n",
    "h2_log_grad = log.backward(label)\n",
    "h2_soft_grad = softmax.backward(h2_log_grad)\n",
    "h2_grad, W2_grad = mul_h2.backward(h2_soft_grad)\n",
    "h1_relu_grad = relu.backward(h2_grad)\n",
    "h1_grad, W1_grad = mul_h1.backward(h1_relu_grad)\n",
    "\n",
    "print(h2_log_grad)\n",
    "print('--'*20)\n",
    "# print(W2_grad)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x, W1, W2, label = tf.constant(x), tf.constant(W1), tf.constant(W2), tf.constant(label)\n",
    "    tape.watch(W1)\n",
    "    tape.watch(W2)\n",
    "    h1 = tf.matmul(x, W1)\n",
    "    h1_relu = tf.nn.relu(h1)\n",
    "    h2 = tf.matmul(h1_relu, W2)\n",
    "    prob = tf.nn.softmax(h2)\n",
    "    log_prob = tf.math.log(prob)\n",
    "    loss = tf.reduce_sum(label * log_prob)\n",
    "    grads = tape.gradient(loss, [prob])\n",
    "    print (grads[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.W1 = np.random.normal(size=[28*28+1, 100])\n",
    "        self.W2 = np.random.normal(size=[100, 10])\n",
    "        \n",
    "        self.mul_h1 = Matmul()\n",
    "        self.mul_h2 = Matmul()\n",
    "        self.relu = Relu()\n",
    "        self.softmax = Softmax()\n",
    "        self.log = Log()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28*28)\n",
    "        bias = np.ones(shape=[x.shape[0], 1])\n",
    "        x = np.concatenate([x, bias], axis=1)\n",
    "        \n",
    "        self.h1 = self.mul_h1.forward(x, self.W1) # shape(5, 4)\n",
    "        self.h1_relu = self.relu.forward(self.h1)\n",
    "        self.h2 = self.mul_h2.forward(self.h1_relu, self.W2)\n",
    "        self.h2_soft = self.softmax.forward(self.h2)\n",
    "        self.h2_log = self.log.forward(self.h2_soft)\n",
    "            \n",
    "    def backward(self, label):\n",
    "        self.h2_log_grad = self.log.backward(-label)\n",
    "        self.h2_soft_grad = self.softmax.backward(self.h2_log_grad)\n",
    "        self.h2_grad, self.W2_grad = self.mul_h2.backward(self.h2_soft_grad)\n",
    "        self.h1_relu_grad = self.relu.backward(self.h2_grad)\n",
    "        self.h1_grad, self.W1_grad = self.mul_h1.backward(self.h1_relu_grad)\n",
    "        \n",
    "model = myModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(log_prob, labels):\n",
    "     return np.mean(np.sum(-log_prob*labels, axis=1))\n",
    "    \n",
    "\n",
    "def compute_accuracy(log_prob, labels):\n",
    "    predictions = np.argmax(log_prob, axis=1)\n",
    "    truth = np.argmax(labels, axis=1)\n",
    "    return np.mean(predictions==truth)\n",
    "\n",
    "def train_one_step(model, x, y):\n",
    "    model.forward(x)\n",
    "    model.backward(y)\n",
    "    model.W1 -= 1e-5* model.W1_grad\n",
    "    model.W2 -= 1e-5* model.W2_grad\n",
    "    loss = compute_loss(model.h2_log, y)\n",
    "    accuracy = compute_accuracy(model.h2_log, y)\n",
    "    return loss, accuracy\n",
    "\n",
    "def test(model, x, y):\n",
    "    model.forward(x)\n",
    "    loss = compute_loss(model.h2_log, y)\n",
    "    accuracy = compute_accuracy(model.h2_log, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 10.08738965302186 ; accuracy 0.6047166666666667\n",
      "epoch 1 : loss 9.949393067869094 ; accuracy 0.6113166666666666\n",
      "epoch 2 : loss 9.920089360722686 ; accuracy 0.6110666666666666\n",
      "epoch 3 : loss 9.819629912836307 ; accuracy 0.6161166666666666\n",
      "epoch 4 : loss 9.747070503041707 ; accuracy 0.6157166666666667\n",
      "epoch 5 : loss 9.584161698681708 ; accuracy 0.6198166666666667\n",
      "epoch 6 : loss 9.23118873253213 ; accuracy 0.6207666666666667\n",
      "epoch 7 : loss 8.968859175376078 ; accuracy 0.6214\n",
      "epoch 8 : loss 8.962067910985581 ; accuracy 0.6319833333333333\n",
      "epoch 9 : loss 9.058689537654056 ; accuracy 0.61515\n",
      "epoch 10 : loss 9.379174120555867 ; accuracy 0.6246333333333334\n",
      "epoch 11 : loss 8.440959025822796 ; accuracy 0.6511833333333333\n",
      "epoch 12 : loss 8.355993650601942 ; accuracy 0.6539\n",
      "epoch 13 : loss 8.237327520049709 ; accuracy 0.6620833333333334\n",
      "epoch 14 : loss 8.423947364918762 ; accuracy 0.6492\n",
      "epoch 15 : loss 8.473342627954738 ; accuracy 0.6575333333333333\n",
      "epoch 16 : loss 8.367818736866312 ; accuracy 0.6515333333333333\n",
      "epoch 17 : loss 8.688224369576135 ; accuracy 0.6523\n",
      "epoch 18 : loss 7.8209359587435525 ; accuracy 0.6789833333333334\n",
      "epoch 19 : loss 7.785671002637201 ; accuracy 0.6837\n",
      "epoch 20 : loss 7.797407686189031 ; accuracy 0.6778166666666666\n",
      "epoch 21 : loss 7.947272336158847 ; accuracy 0.6785166666666667\n",
      "epoch 22 : loss 7.8629376800934025 ; accuracy 0.6753833333333333\n",
      "epoch 23 : loss 8.122454132409098 ; accuracy 0.6722666666666667\n",
      "epoch 24 : loss 7.754297588711708 ; accuracy 0.6810166666666667\n",
      "epoch 25 : loss 7.89964030896563 ; accuracy 0.6805666666666667\n",
      "epoch 26 : loss 7.694400183882798 ; accuracy 0.6838166666666666\n",
      "epoch 27 : loss 7.840347066783064 ; accuracy 0.6833\n",
      "epoch 28 : loss 7.623513520554698 ; accuracy 0.6873666666666667\n",
      "epoch 29 : loss 7.723982114735285 ; accuracy 0.6881666666666667\n",
      "epoch 30 : loss 7.550069147679518 ; accuracy 0.6910333333333334\n",
      "epoch 31 : loss 7.612480421072251 ; accuracy 0.6922166666666667\n",
      "epoch 32 : loss 7.497530033328931 ; accuracy 0.6932833333333334\n",
      "epoch 33 : loss 7.555362392614157 ; accuracy 0.6945166666666667\n",
      "epoch 34 : loss 7.444240968132629 ; accuracy 0.6958333333333333\n",
      "epoch 35 : loss 7.494265011684503 ; accuracy 0.69725\n",
      "epoch 36 : loss 7.402065438426816 ; accuracy 0.69785\n",
      "epoch 37 : loss 7.446897886253734 ; accuracy 0.6992666666666667\n",
      "epoch 38 : loss 7.365214787916614 ; accuracy 0.69935\n",
      "epoch 39 : loss 7.406675788531081 ; accuracy 0.7009333333333333\n",
      "epoch 40 : loss 7.332763949013327 ; accuracy 0.70085\n",
      "epoch 41 : loss 7.370689423142829 ; accuracy 0.7022166666666667\n",
      "epoch 42 : loss 7.302489667093808 ; accuracy 0.7021166666666666\n",
      "epoch 43 : loss 7.335027516336236 ; accuracy 0.70365\n",
      "epoch 44 : loss 7.275168133570835 ; accuracy 0.70365\n",
      "epoch 45 : loss 7.305650680452922 ; accuracy 0.7047333333333333\n",
      "epoch 46 : loss 7.248830232805266 ; accuracy 0.7047166666666667\n",
      "epoch 47 : loss 7.277646118120856 ; accuracy 0.7055666666666667\n",
      "epoch 48 : loss 7.221372781711188 ; accuracy 0.7058\n",
      "epoch 49 : loss 7.24688456080138 ; accuracy 0.7069333333333333\n",
      "epoch 50 : loss 7.191618102814131 ; accuracy 0.7071166666666666\n",
      "epoch 51 : loss 7.212989159592368 ; accuracy 0.7083333333333334\n",
      "epoch 52 : loss 7.160750159279188 ; accuracy 0.7086166666666667\n",
      "epoch 53 : loss 7.178169869921348 ; accuracy 0.7099666666666666\n",
      "epoch 54 : loss 7.130706566558053 ; accuracy 0.70985\n",
      "epoch 55 : loss 7.144277413335375 ; accuracy 0.7112\n",
      "epoch 56 : loss 7.107052112796071 ; accuracy 0.7108333333333333\n",
      "epoch 57 : loss 7.118428477703505 ; accuracy 0.7122833333333334\n",
      "epoch 58 : loss 7.0888384281321635 ; accuracy 0.7113833333333334\n",
      "epoch 59 : loss 7.099861951369606 ; accuracy 0.7126333333333333\n",
      "epoch 60 : loss 7.073889372691025 ; accuracy 0.7122666666666667\n",
      "epoch 61 : loss 7.085178959779576 ; accuracy 0.71335\n",
      "epoch 62 : loss 7.059016977393226 ; accuracy 0.7131333333333333\n",
      "epoch 63 : loss 7.068194295232514 ; accuracy 0.7139333333333333\n",
      "epoch 64 : loss 7.041670555717502 ; accuracy 0.7139833333333333\n",
      "epoch 65 : loss 7.046687577879723 ; accuracy 0.71505\n",
      "epoch 66 : loss 7.020813154902375 ; accuracy 0.7149833333333333\n",
      "epoch 67 : loss 7.020158431289181 ; accuracy 0.7164333333333334\n",
      "epoch 68 : loss 6.996407312717373 ; accuracy 0.7161333333333333\n",
      "epoch 69 : loss 6.990375355969809 ; accuracy 0.7181666666666666\n",
      "epoch 70 : loss 6.96858159879148 ; accuracy 0.7177166666666667\n",
      "epoch 71 : loss 6.95950750642519 ; accuracy 0.7193666666666667\n",
      "epoch 72 : loss 6.93917414444213 ; accuracy 0.71885\n",
      "epoch 73 : loss 6.928616044477316 ; accuracy 0.72045\n",
      "epoch 74 : loss 6.91284862288945 ; accuracy 0.7204\n",
      "epoch 75 : loss 6.902598203922623 ; accuracy 0.7214166666666667\n",
      "epoch 76 : loss 6.891090288655215 ; accuracy 0.7213333333333334\n",
      "epoch 77 : loss 6.882044456082691 ; accuracy 0.7221666666666666\n",
      "epoch 78 : loss 6.872484664875202 ; accuracy 0.7221666666666666\n",
      "epoch 79 : loss 6.864455661010633 ; accuracy 0.7230166666666666\n",
      "epoch 80 : loss 6.855875887608521 ; accuracy 0.7226833333333333\n",
      "epoch 81 : loss 6.848442485868334 ; accuracy 0.72355\n",
      "epoch 82 : loss 6.840544616442465 ; accuracy 0.7233833333333334\n",
      "epoch 83 : loss 6.833706356357125 ; accuracy 0.7241833333333333\n",
      "epoch 84 : loss 6.826462599528946 ; accuracy 0.72395\n",
      "epoch 85 : loss 6.8201053376962415 ; accuracy 0.7248\n",
      "epoch 86 : loss 6.81330539956696 ; accuracy 0.7246833333333333\n",
      "epoch 87 : loss 6.807358496457859 ; accuracy 0.7254\n",
      "epoch 88 : loss 6.8009220116020614 ; accuracy 0.7251833333333333\n",
      "epoch 89 : loss 6.795346996157584 ; accuracy 0.7257666666666667\n",
      "epoch 90 : loss 6.789113627168923 ; accuracy 0.7259666666666666\n",
      "epoch 91 : loss 6.783833853770063 ; accuracy 0.7261\n",
      "epoch 92 : loss 6.777792695827413 ; accuracy 0.72635\n",
      "epoch 93 : loss 6.772878542987587 ; accuracy 0.7266333333333334\n",
      "epoch 94 : loss 6.767101549437721 ; accuracy 0.7267166666666667\n",
      "epoch 95 : loss 6.762809408978377 ; accuracy 0.7272\n",
      "epoch 96 : loss 6.75742851516602 ; accuracy 0.72685\n",
      "epoch 97 : loss 6.754265897033658 ; accuracy 0.72765\n",
      "epoch 98 : loss 6.749759767918507 ; accuracy 0.7273666666666667\n",
      "epoch 99 : loss 6.748861687668779 ; accuracy 0.7278833333333333\n",
      "epoch 100 : loss 6.7462521590455395 ; accuracy 0.7274166666666667\n",
      "epoch 101 : loss 6.750098225953864 ; accuracy 0.7276333333333334\n",
      "epoch 102 : loss 6.750620121845103 ; accuracy 0.7265\n",
      "epoch 103 : loss 6.763570734092484 ; accuracy 0.7268833333333333\n",
      "epoch 104 : loss 6.768056591273017 ; accuracy 0.7251166666666666\n",
      "epoch 105 : loss 6.793382454290279 ; accuracy 0.7252166666666666\n",
      "epoch 106 : loss 6.797032456697652 ; accuracy 0.72345\n",
      "epoch 107 : loss 6.828969611230546 ; accuracy 0.7235333333333334\n",
      "epoch 108 : loss 6.824661437703967 ; accuracy 0.7213666666666667\n",
      "epoch 109 : loss 6.8671196023011065 ; accuracy 0.7217833333333333\n",
      "epoch 110 : loss 6.8445597469796695 ; accuracy 0.7202166666666666\n",
      "epoch 111 : loss 6.8773755171453255 ; accuracy 0.72115\n",
      "epoch 112 : loss 6.834279354560864 ; accuracy 0.7208833333333333\n",
      "epoch 113 : loss 6.842179208356098 ; accuracy 0.7233333333333334\n",
      "epoch 114 : loss 6.777607550633487 ; accuracy 0.7243\n",
      "epoch 115 : loss 6.761424716226661 ; accuracy 0.7274333333333334\n",
      "epoch 116 : loss 6.709538953498069 ; accuracy 0.7285666666666667\n",
      "epoch 117 : loss 6.693022606431216 ; accuracy 0.7308333333333333\n",
      "epoch 118 : loss 6.661203955857108 ; accuracy 0.7319\n",
      "epoch 119 : loss 6.645376700791635 ; accuracy 0.7330166666666666\n",
      "epoch 120 : loss 6.6319265457528775 ; accuracy 0.7338333333333333\n",
      "epoch 121 : loss 6.623027163466015 ; accuracy 0.7337833333333333\n",
      "epoch 122 : loss 6.615901428254686 ; accuracy 0.73445\n",
      "epoch 123 : loss 6.609605705960171 ; accuracy 0.7342666666666666\n",
      "epoch 124 : loss 6.604639735609604 ; accuracy 0.7348\n",
      "epoch 125 : loss 6.599380145067439 ; accuracy 0.73485\n",
      "epoch 126 : loss 6.594964306412634 ; accuracy 0.7351166666666666\n",
      "epoch 127 : loss 6.590131861797977 ; accuracy 0.7352\n",
      "epoch 128 : loss 6.585997079701154 ; accuracy 0.7354833333333334\n",
      "epoch 129 : loss 6.581388653002682 ; accuracy 0.7354666666666667\n",
      "epoch 130 : loss 6.577534931456194 ; accuracy 0.7357\n",
      "epoch 131 : loss 6.573095329026815 ; accuracy 0.7357166666666667\n",
      "epoch 132 : loss 6.569590306844056 ; accuracy 0.7359666666666667\n",
      "epoch 133 : loss 6.565374329576292 ; accuracy 0.7362\n",
      "epoch 134 : loss 6.562376713232072 ; accuracy 0.7362833333333333\n",
      "epoch 135 : loss 6.558471679073227 ; accuracy 0.7363166666666666\n",
      "epoch 136 : loss 6.556254103537185 ; accuracy 0.7362166666666666\n",
      "epoch 137 : loss 6.552779997582106 ; accuracy 0.7364333333333334\n",
      "epoch 138 : loss 6.551608478620491 ; accuracy 0.7366\n",
      "epoch 139 : loss 6.548680690841072 ; accuracy 0.7365333333333334\n",
      "epoch 140 : loss 6.5484027620729535 ; accuracy 0.7365666666666667\n",
      "epoch 141 : loss 6.5459173447393635 ; accuracy 0.7366666666666667\n",
      "epoch 142 : loss 6.546292172024092 ; accuracy 0.73615\n",
      "epoch 143 : loss 6.544647412505105 ; accuracy 0.7366666666666667\n",
      "epoch 144 : loss 6.545384761200795 ; accuracy 0.7359833333333333\n",
      "epoch 145 : loss 6.544653306842999 ; accuracy 0.73655\n",
      "epoch 146 : loss 6.54559372233505 ; accuracy 0.7357\n",
      "epoch 147 : loss 6.545942701409237 ; accuracy 0.7364666666666667\n",
      "epoch 148 : loss 6.5466121677250575 ; accuracy 0.7356\n",
      "epoch 149 : loss 6.550299809304873 ; accuracy 0.7360166666666667\n",
      "epoch 150 : loss 6.54921657079085 ; accuracy 0.73515\n",
      "epoch 151 : loss 6.5592309494340855 ; accuracy 0.7355166666666667\n",
      "epoch 152 : loss 6.557664891322405 ; accuracy 0.7342333333333333\n",
      "epoch 153 : loss 6.576218668322748 ; accuracy 0.7343333333333333\n",
      "epoch 154 : loss 6.572851206550492 ; accuracy 0.7331833333333333\n",
      "epoch 155 : loss 6.600592276776631 ; accuracy 0.7327833333333333\n",
      "epoch 156 : loss 6.594656966374495 ; accuracy 0.7318166666666667\n",
      "epoch 157 : loss 6.636402003980692 ; accuracy 0.73115\n",
      "epoch 158 : loss 6.636223100581082 ; accuracy 0.7290833333333333\n",
      "epoch 159 : loss 6.698965565440909 ; accuracy 0.7271333333333333\n",
      "epoch 160 : loss 6.708408173978237 ; accuracy 0.72435\n",
      "epoch 161 : loss 6.813126307679313 ; accuracy 0.7216666666666667\n",
      "epoch 162 : loss 6.811981897924802 ; accuracy 0.7187833333333333\n",
      "epoch 163 : loss 6.929058849772622 ; accuracy 0.7160333333333333\n",
      "epoch 164 : loss 6.8462066841289735 ; accuracy 0.7169166666666666\n",
      "epoch 165 : loss 6.864498234119338 ; accuracy 0.72\n",
      "epoch 166 : loss 6.678332375658885 ; accuracy 0.7279666666666667\n",
      "epoch 167 : loss 6.597154894180849 ; accuracy 0.7346166666666667\n",
      "epoch 168 : loss 6.501081233322153 ; accuracy 0.7389166666666667\n",
      "epoch 169 : loss 6.47370366196467 ; accuracy 0.74085\n",
      "epoch 170 : loss 6.461172056840395 ; accuracy 0.7411\n",
      "epoch 171 : loss 6.454770596023704 ; accuracy 0.74215\n",
      "epoch 172 : loss 6.449847431664858 ; accuracy 0.74185\n",
      "epoch 173 : loss 6.44572189253014 ; accuracy 0.74235\n",
      "epoch 174 : loss 6.441828179840859 ; accuracy 0.74235\n",
      "epoch 175 : loss 6.438072164006671 ; accuracy 0.7424833333333334\n",
      "epoch 176 : loss 6.434406077757299 ; accuracy 0.7427\n",
      "epoch 177 : loss 6.430819857891093 ; accuracy 0.7426666666666667\n",
      "epoch 178 : loss 6.4273040219403095 ; accuracy 0.7427666666666667\n",
      "epoch 179 : loss 6.423852770751043 ; accuracy 0.7428666666666667\n",
      "epoch 180 : loss 6.420457504078341 ; accuracy 0.7429666666666667\n",
      "epoch 181 : loss 6.417108095386333 ; accuracy 0.7431333333333333\n",
      "epoch 182 : loss 6.413802119851905 ; accuracy 0.7433166666666666\n",
      "epoch 183 : loss 6.4105379658196355 ; accuracy 0.74335\n",
      "epoch 184 : loss 6.4073100297309145 ; accuracy 0.7434\n",
      "epoch 185 : loss 6.4041164063276295 ; accuracy 0.7434666666666667\n",
      "epoch 186 : loss 6.400956975041989 ; accuracy 0.7436666666666667\n",
      "epoch 187 : loss 6.397824410418824 ; accuracy 0.7437666666666667\n",
      "epoch 188 : loss 6.394712108340519 ; accuracy 0.7439333333333333\n",
      "epoch 189 : loss 6.391635640943021 ; accuracy 0.7438333333333333\n",
      "epoch 190 : loss 6.388599706086872 ; accuracy 0.7441666666666666\n",
      "epoch 191 : loss 6.385617168272616 ; accuracy 0.7439833333333333\n",
      "epoch 192 : loss 6.382742698343015 ; accuracy 0.7442333333333333\n",
      "epoch 193 : loss 6.379971906695758 ; accuracy 0.74415\n",
      "epoch 194 : loss 6.377438544643003 ; accuracy 0.7442833333333333\n",
      "epoch 195 : loss 6.375122746446884 ; accuracy 0.7441166666666666\n",
      "epoch 196 : loss 6.373175478032204 ; accuracy 0.7441333333333333\n",
      "epoch 197 : loss 6.371657592349955 ; accuracy 0.7442\n",
      "epoch 198 : loss 6.370341090288525 ; accuracy 0.7441\n",
      "epoch 199 : loss 6.369435045771228 ; accuracy 0.7438166666666667\n",
      "epoch 200 : loss 6.368324691048279 ; accuracy 0.7439333333333333\n",
      "epoch 201 : loss 6.367024241014147 ; accuracy 0.74345\n",
      "epoch 202 : loss 6.365181121731495 ; accuracy 0.7438166666666667\n",
      "epoch 203 : loss 6.362872423714034 ; accuracy 0.7433333333333333\n",
      "epoch 204 : loss 6.359617509798554 ; accuracy 0.74375\n",
      "epoch 205 : loss 6.355875397636107 ; accuracy 0.7434666666666667\n",
      "epoch 206 : loss 6.351272183684505 ; accuracy 0.7435166666666667\n",
      "epoch 207 : loss 6.345753732475144 ; accuracy 0.7438166666666667\n",
      "epoch 208 : loss 6.3394947571673805 ; accuracy 0.7434\n",
      "epoch 209 : loss 6.331077440134428 ; accuracy 0.7437833333333334\n",
      "epoch 210 : loss 6.319259484370818 ; accuracy 0.7436166666666667\n",
      "epoch 211 : loss 6.303332376740234 ; accuracy 0.74375\n",
      "epoch 212 : loss 6.280050370701662 ; accuracy 0.74385\n",
      "epoch 213 : loss 6.238425277638383 ; accuracy 0.74315\n",
      "epoch 214 : loss 6.163144018113652 ; accuracy 0.7446\n",
      "epoch 215 : loss 6.028525141210818 ; accuracy 0.7444333333333333\n",
      "epoch 216 : loss 5.806206119249298 ; accuracy 0.7477833333333334\n",
      "epoch 217 : loss 5.521013185274663 ; accuracy 0.74955\n",
      "epoch 218 : loss 5.265655138665081 ; accuracy 0.7601\n",
      "epoch 219 : loss 5.394018008197202 ; accuracy 0.7492666666666666\n",
      "epoch 220 : loss 5.739816311024862 ; accuracy 0.7346166666666667\n",
      "epoch 221 : loss 6.627467606242533 ; accuracy 0.7186\n",
      "epoch 222 : loss 5.039748468040998 ; accuracy 0.7706\n",
      "epoch 223 : loss 4.618523934218199 ; accuracy 0.7915\n",
      "epoch 224 : loss 4.499155262099088 ; accuracy 0.79525\n",
      "epoch 225 : loss 4.373754400133917 ; accuracy 0.8033\n",
      "epoch 226 : loss 4.301278419451422 ; accuracy 0.8062833333333334\n",
      "epoch 227 : loss 4.240936223193721 ; accuracy 0.8100666666666667\n",
      "epoch 228 : loss 4.196396492659684 ; accuracy 0.8115166666666667\n",
      "epoch 229 : loss 4.163860829191673 ; accuracy 0.8134\n",
      "epoch 230 : loss 4.134639432513895 ; accuracy 0.8149166666666666\n",
      "epoch 231 : loss 4.111008062772218 ; accuracy 0.8158833333333333\n",
      "epoch 232 : loss 4.0889699492781295 ; accuracy 0.817\n",
      "epoch 233 : loss 4.069905915089432 ; accuracy 0.8178166666666666\n",
      "epoch 234 : loss 4.051634011027601 ; accuracy 0.8191\n",
      "epoch 235 : loss 4.035227854964855 ; accuracy 0.81965\n",
      "epoch 236 : loss 4.019871542455197 ; accuracy 0.8208833333333333\n",
      "epoch 237 : loss 4.005926504395271 ; accuracy 0.8215166666666667\n",
      "epoch 238 : loss 3.992768604020935 ; accuracy 0.8225333333333333\n",
      "epoch 239 : loss 3.980675087277495 ; accuracy 0.82275\n",
      "epoch 240 : loss 3.969250094297117 ; accuracy 0.8237166666666667\n",
      "epoch 241 : loss 3.9587313842358296 ; accuracy 0.8239\n",
      "epoch 242 : loss 3.9488160744704985 ; accuracy 0.8245333333333333\n",
      "epoch 243 : loss 3.9395749682126047 ; accuracy 0.8246833333333333\n",
      "epoch 244 : loss 3.9307866974691894 ; accuracy 0.8256666666666667\n",
      "epoch 245 : loss 3.9225100283372725 ; accuracy 0.8255166666666667\n",
      "epoch 246 : loss 3.9146173644190463 ; accuracy 0.82645\n",
      "epoch 247 : loss 3.9071910584062324 ; accuracy 0.8262666666666667\n",
      "epoch 248 : loss 3.900098930228434 ; accuracy 0.8271666666666667\n",
      "epoch 249 : loss 3.893439010629235 ; accuracy 0.8267\n",
      "epoch 250 : loss 3.887071828518029 ; accuracy 0.82785\n",
      "epoch 251 : loss 3.881123382531133 ; accuracy 0.8272333333333334\n",
      "epoch 252 : loss 3.8755460761457345 ; accuracy 0.82855\n",
      "epoch 253 : loss 3.870451972532205 ; accuracy 0.8276333333333333\n",
      "epoch 254 : loss 3.865922311043894 ; accuracy 0.8291\n",
      "epoch 255 : loss 3.8620539919405985 ; accuracy 0.8279\n",
      "epoch 256 : loss 3.859250067271327 ; accuracy 0.8293666666666667\n",
      "epoch 257 : loss 3.857276688135395 ; accuracy 0.8279333333333333\n",
      "epoch 258 : loss 3.857479028761943 ; accuracy 0.8290666666666666\n",
      "epoch 259 : loss 3.8580947162168444 ; accuracy 0.8273666666666667\n",
      "epoch 260 : loss 3.8646061671969387 ; accuracy 0.8278166666666666\n",
      "epoch 261 : loss 3.8709741153303794 ; accuracy 0.8265333333333333\n",
      "epoch 262 : loss 3.8900374216852383 ; accuracy 0.8253333333333334\n",
      "epoch 263 : loss 3.906940793533059 ; accuracy 0.82415\n",
      "epoch 264 : loss 3.9521257496739484 ; accuracy 0.8215833333333333\n",
      "epoch 265 : loss 3.980824941376487 ; accuracy 0.8187833333333333\n",
      "epoch 266 : loss 4.096563823581039 ; accuracy 0.8128833333333333\n",
      "epoch 267 : loss 4.1829534514843045 ; accuracy 0.8057666666666666\n",
      "epoch 268 : loss 4.529478468572639 ; accuracy 0.79205\n",
      "epoch 269 : loss 4.601848291428831 ; accuracy 0.7828166666666667\n",
      "epoch 270 : loss 5.060917241232071 ; accuracy 0.7716666666666666\n",
      "epoch 271 : loss 4.201768097237195 ; accuracy 0.8069833333333334\n",
      "epoch 272 : loss 4.023977599969225 ; accuracy 0.8204666666666667\n",
      "epoch 273 : loss 3.84434964693907 ; accuracy 0.8302\n",
      "epoch 274 : loss 3.8145008155450957 ; accuracy 0.83185\n",
      "epoch 275 : loss 3.8017284826678943 ; accuracy 0.8326833333333333\n",
      "epoch 276 : loss 3.7939790949475873 ; accuracy 0.8326666666666667\n",
      "epoch 277 : loss 3.787541303726582 ; accuracy 0.8329666666666666\n",
      "epoch 278 : loss 3.7817708217768744 ; accuracy 0.8332333333333334\n",
      "epoch 279 : loss 3.776382701454784 ; accuracy 0.83355\n",
      "epoch 280 : loss 3.7712793116069094 ; accuracy 0.8337666666666667\n",
      "epoch 281 : loss 3.7664074145006907 ; accuracy 0.834\n",
      "epoch 282 : loss 3.7617357029431733 ; accuracy 0.8342333333333334\n",
      "epoch 283 : loss 3.7572283842447836 ; accuracy 0.8345166666666667\n",
      "epoch 284 : loss 3.7528617797411292 ; accuracy 0.83465\n",
      "epoch 285 : loss 3.748629972972254 ; accuracy 0.8347833333333333\n",
      "epoch 286 : loss 3.744518804557796 ; accuracy 0.8349166666666666\n",
      "epoch 287 : loss 3.740513682714917 ; accuracy 0.8349333333333333\n",
      "epoch 288 : loss 3.7366151447015503 ; accuracy 0.8349333333333333\n",
      "epoch 289 : loss 3.7328079130218703 ; accuracy 0.8351166666666666\n",
      "epoch 290 : loss 3.72908014223313 ; accuracy 0.8353666666666667\n",
      "epoch 291 : loss 3.7254239000377964 ; accuracy 0.8355666666666667\n",
      "epoch 292 : loss 3.7218383687872603 ; accuracy 0.8357666666666667\n",
      "epoch 293 : loss 3.718320683443033 ; accuracy 0.8357833333333333\n",
      "epoch 294 : loss 3.7148655654347973 ; accuracy 0.8358166666666667\n",
      "epoch 295 : loss 3.7114748921501803 ; accuracy 0.8359666666666666\n",
      "epoch 296 : loss 3.7081445155026724 ; accuracy 0.8360666666666666\n",
      "epoch 297 : loss 3.7048737657341144 ; accuracy 0.8361333333333333\n",
      "epoch 298 : loss 3.701660949575787 ; accuracy 0.8362833333333334\n",
      "epoch 299 : loss 3.698503804145814 ; accuracy 0.8364333333333334\n",
      "epoch 300 : loss 3.695398011583233 ; accuracy 0.83665\n",
      "epoch 301 : loss 3.6923384531971473 ; accuracy 0.83685\n",
      "epoch 302 : loss 3.68932022256319 ; accuracy 0.8370833333333333\n",
      "epoch 303 : loss 3.686348589897834 ; accuracy 0.8371833333333333\n",
      "epoch 304 : loss 3.6834182614840683 ; accuracy 0.8372666666666667\n",
      "epoch 305 : loss 3.6805298769694956 ; accuracy 0.8373833333333334\n",
      "epoch 306 : loss 3.6776819310903295 ; accuracy 0.8374333333333334\n",
      "epoch 307 : loss 3.6748726351343257 ; accuracy 0.8375833333333333\n",
      "epoch 308 : loss 3.6721009166541636 ; accuracy 0.83765\n",
      "epoch 309 : loss 3.6693646040206382 ; accuracy 0.8377666666666667\n",
      "epoch 310 : loss 3.6666649737601404 ; accuracy 0.8380833333333333\n",
      "epoch 311 : loss 3.6639945705590047 ; accuracy 0.83825\n",
      "epoch 312 : loss 3.6613518082417116 ; accuracy 0.8382833333333334\n",
      "epoch 313 : loss 3.6587355732702695 ; accuracy 0.8385\n",
      "epoch 314 : loss 3.656142774133836 ; accuracy 0.83865\n",
      "epoch 315 : loss 3.653578268719342 ; accuracy 0.8387666666666667\n",
      "epoch 316 : loss 3.651043896492265 ; accuracy 0.8388166666666667\n",
      "epoch 317 : loss 3.6485341175087123 ; accuracy 0.8388666666666666\n",
      "epoch 318 : loss 3.6460502549616107 ; accuracy 0.8389666666666666\n",
      "epoch 319 : loss 3.643587277599443 ; accuracy 0.83905\n",
      "epoch 320 : loss 3.6411470473258287 ; accuracy 0.8391\n",
      "epoch 321 : loss 3.6387301570343076 ; accuracy 0.83915\n",
      "epoch 322 : loss 3.6363377964149843 ; accuracy 0.8392833333333334\n",
      "epoch 323 : loss 3.633965819755986 ; accuracy 0.8393166666666667\n",
      "epoch 324 : loss 3.63161856313486 ; accuracy 0.8395\n",
      "epoch 325 : loss 3.6292951629529693 ; accuracy 0.8395333333333334\n",
      "epoch 326 : loss 3.6269979699671175 ; accuracy 0.83975\n",
      "epoch 327 : loss 3.624725496986758 ; accuracy 0.8397666666666667\n",
      "epoch 328 : loss 3.6224799937726413 ; accuracy 0.8399833333333333\n",
      "epoch 329 : loss 3.620271889353594 ; accuracy 0.8398833333333333\n",
      "epoch 330 : loss 3.6181087542641777 ; accuracy 0.8400333333333333\n",
      "epoch 331 : loss 3.616007536501356 ; accuracy 0.8398666666666667\n",
      "epoch 332 : loss 3.613995938912649 ; accuracy 0.8404\n",
      "epoch 333 : loss 3.6121239707921005 ; accuracy 0.8401\n",
      "epoch 334 : loss 3.6104680116314016 ; accuracy 0.84045\n",
      "epoch 335 : loss 3.6091569873731215 ; accuracy 0.8401\n",
      "epoch 336 : loss 3.6084113657847436 ; accuracy 0.8402666666666667\n",
      "epoch 337 : loss 3.6083664187961957 ; accuracy 0.8396166666666667\n",
      "epoch 338 : loss 3.6096579561186584 ; accuracy 0.8401166666666666\n",
      "epoch 339 : loss 3.61172304132773 ; accuracy 0.83905\n",
      "epoch 340 : loss 3.61709497439869 ; accuracy 0.8391666666666666\n",
      "epoch 341 : loss 3.622710172222609 ; accuracy 0.8378666666666666\n",
      "epoch 342 : loss 3.638272500453372 ; accuracy 0.8367666666666667\n",
      "epoch 343 : loss 3.6524363337886183 ; accuracy 0.8353833333333334\n",
      "epoch 344 : loss 3.683276346037832 ; accuracy 0.8331166666666666\n",
      "epoch 345 : loss 3.7131906418547174 ; accuracy 0.8311333333333333\n",
      "epoch 346 : loss 3.775080263002203 ; accuracy 0.8272\n",
      "epoch 347 : loss 3.8192488777797364 ; accuracy 0.8236666666666667\n",
      "epoch 348 : loss 3.9702266173593177 ; accuracy 0.8151166666666667\n",
      "epoch 349 : loss 4.089840124264086 ; accuracy 0.8055333333333333\n",
      "epoch 350 : loss 4.5397268219549485 ; accuracy 0.7884833333333333\n",
      "epoch 351 : loss 4.466432907224759 ; accuracy 0.7845166666666666\n",
      "epoch 352 : loss 4.816265578141087 ; accuracy 0.7799333333333334\n",
      "epoch 353 : loss 3.8950401232560785 ; accuracy 0.8211\n",
      "epoch 354 : loss 3.700170361955556 ; accuracy 0.8362833333333334\n",
      "epoch 355 : loss 3.622555275659691 ; accuracy 0.8399333333333333\n",
      "epoch 356 : loss 3.6105733746781583 ; accuracy 0.8411666666666666\n",
      "epoch 357 : loss 3.6040657366888618 ; accuracy 0.8411833333333333\n",
      "epoch 358 : loss 3.598768444985051 ; accuracy 0.8413166666666667\n",
      "epoch 359 : loss 3.5940081285145444 ; accuracy 0.8414\n",
      "epoch 360 : loss 3.5896664047640536 ; accuracy 0.8417833333333333\n",
      "epoch 361 : loss 3.5856753856151613 ; accuracy 0.8418833333333333\n",
      "epoch 362 : loss 3.581980868005043 ; accuracy 0.8420666666666666\n",
      "epoch 363 : loss 3.578543769192116 ; accuracy 0.84225\n",
      "epoch 364 : loss 3.575320125000497 ; accuracy 0.84245\n",
      "epoch 365 : loss 3.5722783309288797 ; accuracy 0.8426166666666667\n",
      "epoch 366 : loss 3.569393145133999 ; accuracy 0.84265\n",
      "epoch 367 : loss 3.5666467869316683 ; accuracy 0.8427666666666667\n",
      "epoch 368 : loss 3.56402236433279 ; accuracy 0.8426833333333333\n",
      "epoch 369 : loss 3.561503480105634 ; accuracy 0.8428166666666667\n",
      "epoch 370 : loss 3.5590767498478426 ; accuracy 0.8429166666666666\n",
      "epoch 371 : loss 3.5567293183007966 ; accuracy 0.84305\n",
      "epoch 372 : loss 3.5544546042281784 ; accuracy 0.8430333333333333\n",
      "epoch 373 : loss 3.5522477275531337 ; accuracy 0.8431333333333333\n",
      "epoch 374 : loss 3.5500993189807626 ; accuracy 0.8432333333333333\n",
      "epoch 375 : loss 3.5480025096440433 ; accuracy 0.84335\n",
      "epoch 376 : loss 3.5459518098610863 ; accuracy 0.8434666666666667\n",
      "epoch 377 : loss 3.5439432547245433 ; accuracy 0.8436333333333333\n",
      "epoch 378 : loss 3.541971257251931 ; accuracy 0.8437333333333333\n",
      "epoch 379 : loss 3.5400325402449795 ; accuracy 0.8437666666666667\n",
      "epoch 380 : loss 3.538123904132955 ; accuracy 0.8438666666666667\n",
      "epoch 381 : loss 3.536244271954225 ; accuracy 0.8438333333333333\n",
      "epoch 382 : loss 3.534391556085616 ; accuracy 0.8439333333333333\n",
      "epoch 383 : loss 3.5325646204963723 ; accuracy 0.8441\n",
      "epoch 384 : loss 3.5307611924508406 ; accuracy 0.8441333333333333\n",
      "epoch 385 : loss 3.5289799437399 ; accuracy 0.8441166666666666\n",
      "epoch 386 : loss 3.5272219612151825 ; accuracy 0.8440666666666666\n",
      "epoch 387 : loss 3.525482409459212 ; accuracy 0.8440666666666666\n",
      "epoch 388 : loss 3.523758649106739 ; accuracy 0.8440666666666666\n",
      "epoch 389 : loss 3.522052391996976 ; accuracy 0.8441166666666666\n",
      "epoch 390 : loss 3.520362528293639 ; accuracy 0.8441666666666666\n",
      "epoch 391 : loss 3.518686741559191 ; accuracy 0.8442833333333334\n",
      "epoch 392 : loss 3.5170243697342287 ; accuracy 0.8443833333333334\n",
      "epoch 393 : loss 3.515378247920042 ; accuracy 0.84455\n",
      "epoch 394 : loss 3.5137477605792586 ; accuracy 0.8445833333333334\n",
      "epoch 395 : loss 3.5121297703857173 ; accuracy 0.8445333333333334\n",
      "epoch 396 : loss 3.5105264702964454 ; accuracy 0.8446333333333333\n",
      "epoch 397 : loss 3.5089361292811447 ; accuracy 0.8446833333333333\n",
      "epoch 398 : loss 3.507357350620287 ; accuracy 0.8448\n",
      "epoch 399 : loss 3.505790679077666 ; accuracy 0.8448833333333333\n",
      "epoch 400 : loss 3.5042319933194177 ; accuracy 0.8449\n",
      "epoch 401 : loss 3.5026823671816976 ; accuracy 0.8449833333333333\n",
      "epoch 402 : loss 3.5011473444119456 ; accuracy 0.84505\n",
      "epoch 403 : loss 3.499623744515462 ; accuracy 0.8451\n",
      "epoch 404 : loss 3.498112159965444 ; accuracy 0.8451\n",
      "epoch 405 : loss 3.4966117533314796 ; accuracy 0.8451333333333333\n",
      "epoch 406 : loss 3.4951205294775796 ; accuracy 0.8451666666666666\n",
      "epoch 407 : loss 3.493638159719023 ; accuracy 0.8452333333333333\n",
      "epoch 408 : loss 3.49216448098478 ; accuracy 0.8452833333333334\n",
      "epoch 409 : loss 3.490700959057045 ; accuracy 0.8453833333333334\n",
      "epoch 410 : loss 3.489246836077469 ; accuracy 0.84545\n",
      "epoch 411 : loss 3.4878022979140346 ; accuracy 0.8455666666666667\n",
      "epoch 412 : loss 3.486368124942298 ; accuracy 0.8456166666666667\n",
      "epoch 413 : loss 3.4849433801344865 ; accuracy 0.8456833333333333\n",
      "epoch 414 : loss 3.48352809192174 ; accuracy 0.8458166666666667\n",
      "epoch 415 : loss 3.482124369690219 ; accuracy 0.8458666666666667\n",
      "epoch 416 : loss 3.4807299260384084 ; accuracy 0.84595\n",
      "epoch 417 : loss 3.4793442267347565 ; accuracy 0.8460666666666666\n",
      "epoch 418 : loss 3.477966172354461 ; accuracy 0.8460333333333333\n",
      "epoch 419 : loss 3.476595785939056 ; accuracy 0.8462333333333333\n",
      "epoch 420 : loss 3.4752341019445954 ; accuracy 0.8462333333333333\n",
      "epoch 421 : loss 3.473882897357517 ; accuracy 0.8465166666666667\n",
      "epoch 422 : loss 3.472540332570014 ; accuracy 0.8463666666666667\n",
      "epoch 423 : loss 3.47120809877491 ; accuracy 0.8466833333333333\n",
      "epoch 424 : loss 3.469887645338929 ; accuracy 0.8465666666666667\n",
      "epoch 425 : loss 3.4685887530261046 ; accuracy 0.8466833333333333\n",
      "epoch 426 : loss 3.467318420324327 ; accuracy 0.8466166666666667\n",
      "epoch 427 : loss 3.466098527391549 ; accuracy 0.8467833333333333\n",
      "epoch 428 : loss 3.464947645611407 ; accuracy 0.8465333333333334\n",
      "epoch 429 : loss 3.46393809757777 ; accuracy 0.84685\n",
      "epoch 430 : loss 3.463092961627042 ; accuracy 0.84645\n",
      "epoch 431 : loss 3.462644174482772 ; accuracy 0.8470666666666666\n",
      "epoch 432 : loss 3.462465047658732 ; accuracy 0.8463\n",
      "epoch 433 : loss 3.4632509492920343 ; accuracy 0.8464833333333334\n",
      "epoch 434 : loss 3.4641108904850184 ; accuracy 0.8457666666666667\n",
      "epoch 435 : loss 3.4669718826800455 ; accuracy 0.8458\n",
      "epoch 436 : loss 3.4692247334083524 ; accuracy 0.8452\n",
      "epoch 437 : loss 3.474953229033079 ; accuracy 0.845\n",
      "epoch 438 : loss 3.4791334400117973 ; accuracy 0.84435\n",
      "epoch 439 : loss 3.488975302366392 ; accuracy 0.84345\n",
      "epoch 440 : loss 3.496074576475912 ; accuracy 0.8426333333333333\n",
      "epoch 441 : loss 3.5133570429385457 ; accuracy 0.84075\n",
      "epoch 442 : loss 3.5252108979386447 ; accuracy 0.8399333333333333\n",
      "epoch 443 : loss 3.552475806290337 ; accuracy 0.8378\n",
      "epoch 444 : loss 3.5656338927251463 ; accuracy 0.8367666666666667\n",
      "epoch 445 : loss 3.6063216281120676 ; accuracy 0.8339166666666666\n",
      "epoch 446 : loss 3.6117054259228163 ; accuracy 0.8334833333333334\n",
      "epoch 447 : loss 3.662197333695234 ; accuracy 0.8297333333333333\n",
      "epoch 448 : loss 3.6576490175089957 ; accuracy 0.8301\n",
      "epoch 449 : loss 3.7148338729993235 ; accuracy 0.8266666666666667\n",
      "epoch 450 : loss 3.6847460824968112 ; accuracy 0.8283833333333334\n",
      "epoch 451 : loss 3.7253874492392653 ; accuracy 0.8265666666666667\n",
      "epoch 452 : loss 3.6400858891024606 ; accuracy 0.8311833333333334\n",
      "epoch 453 : loss 3.6272325668877317 ; accuracy 0.83385\n",
      "epoch 454 : loss 3.5234915217270544 ; accuracy 0.8405\n",
      "epoch 455 : loss 3.4942884165660506 ; accuracy 0.8440666666666666\n",
      "epoch 456 : loss 3.4593738066675757 ; accuracy 0.8461\n",
      "epoch 457 : loss 3.449995577144434 ; accuracy 0.84775\n",
      "epoch 458 : loss 3.4429383274367464 ; accuracy 0.8474333333333334\n",
      "epoch 459 : loss 3.4392066535538586 ; accuracy 0.8484166666666667\n",
      "epoch 460 : loss 3.4361073232988866 ; accuracy 0.84805\n",
      "epoch 461 : loss 3.4337964396340035 ; accuracy 0.84875\n",
      "epoch 462 : loss 3.431725833109801 ; accuracy 0.8480833333333333\n",
      "epoch 463 : loss 3.429907485922189 ; accuracy 0.8488166666666667\n",
      "epoch 464 : loss 3.428206672387669 ; accuracy 0.8482833333333333\n",
      "epoch 465 : loss 3.4266130601103746 ; accuracy 0.8489\n",
      "epoch 466 : loss 3.425085225945411 ; accuracy 0.8485166666666667\n",
      "epoch 467 : loss 3.423612344794919 ; accuracy 0.8489333333333333\n",
      "epoch 468 : loss 3.4221800467605203 ; accuracy 0.8486666666666667\n",
      "epoch 469 : loss 3.420783783952958 ; accuracy 0.849\n",
      "epoch 470 : loss 3.419415140272548 ; accuracy 0.8488166666666667\n",
      "epoch 471 : loss 3.4180742643722377 ; accuracy 0.84915\n",
      "epoch 472 : loss 3.4167519493302927 ; accuracy 0.8489333333333333\n",
      "epoch 473 : loss 3.4154516859010178 ; accuracy 0.84935\n",
      "epoch 474 : loss 3.4141681359074036 ; accuracy 0.849\n",
      "epoch 475 : loss 3.412907176730916 ; accuracy 0.84935\n",
      "epoch 476 : loss 3.41165922292395 ; accuracy 0.8490833333333333\n",
      "epoch 477 : loss 3.4104345326185053 ; accuracy 0.84945\n",
      "epoch 478 : loss 3.4092235845766345 ; accuracy 0.84915\n",
      "epoch 479 : loss 3.408044298780691 ; accuracy 0.8495166666666667\n",
      "epoch 480 : loss 3.406881587273685 ; accuracy 0.8491833333333333\n",
      "epoch 481 : loss 3.4057693670672586 ; accuracy 0.8495833333333334\n",
      "epoch 482 : loss 3.4046755476983614 ; accuracy 0.8491333333333333\n",
      "epoch 483 : loss 3.403668676042954 ; accuracy 0.8496\n",
      "epoch 484 : loss 3.4026791911365675 ; accuracy 0.8490666666666666\n",
      "epoch 485 : loss 3.40184148314205 ; accuracy 0.8496\n",
      "epoch 486 : loss 3.4010024824124683 ; accuracy 0.8488666666666667\n",
      "epoch 487 : loss 3.4004166596261722 ; accuracy 0.8495833333333334\n",
      "epoch 488 : loss 3.399773411958505 ; accuracy 0.8488666666666667\n",
      "epoch 489 : loss 3.3995420778782934 ; accuracy 0.84915\n",
      "epoch 490 : loss 3.399139961846356 ; accuracy 0.8488166666666667\n",
      "epoch 491 : loss 3.399358843868074 ; accuracy 0.8488333333333333\n",
      "epoch 492 : loss 3.3992647000628713 ; accuracy 0.8484666666666667\n",
      "epoch 493 : loss 3.4000816012007196 ; accuracy 0.8483666666666667\n",
      "epoch 494 : loss 3.400369910789245 ; accuracy 0.8480333333333333\n",
      "epoch 495 : loss 3.4019537118690923 ; accuracy 0.84775\n",
      "epoch 496 : loss 3.4026212836963783 ; accuracy 0.8474333333333334\n",
      "epoch 497 : loss 3.405129360302609 ; accuracy 0.8472166666666666\n",
      "epoch 498 : loss 3.406015095407894 ; accuracy 0.8469666666666666\n",
      "epoch 499 : loss 3.40968853810685 ; accuracy 0.84655\n",
      "test loss 3.583465386129563 ; accuracy 0.8385\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "train_label = np.zeros(shape=[train_data[0].shape[0], 10])\n",
    "test_label = np.zeros(shape=[test_data[0].shape[0], 10])\n",
    "train_label[np.arange(train_data[0].shape[0]), np.array(train_data[1])] = 1.\n",
    "test_label[np.arange(test_data[0].shape[0]), np.array(test_data[1])] = 1.\n",
    "\n",
    "for epoch in range(500):\n",
    "    loss, accuracy = train_one_step(model, train_data[0], train_label)\n",
    "    print('epoch', epoch, ': loss', loss, '; accuracy', accuracy)\n",
    "loss, accuracy = test(model, test_data[0], test_label)\n",
    "\n",
    "print('test loss', loss, '; accuracy', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
